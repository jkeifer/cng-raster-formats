{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d50b3a-1ed7-437e-bf35-c9b257eb60eb",
   "metadata": {},
   "source": [
    "# Understanding Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e656e-3872-46a5-9f11-12674cacc540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import planetary_computer\n",
    "import pystac_client\n",
    "import pyproj\n",
    "\n",
    "from odc.geo.geom import point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810039d2-7f44-488c-917d-821845fad657",
   "metadata": {},
   "source": [
    "# Point of Interest (POI)\n",
    "\n",
    "I want to go to Puerto Rico, but I'm from a northern temporate latitude. I'd like to find the best week of the year for me to visit Puerto Rico without having oppressive weather. Can we use a zarr dataset from Planetary Computer to answer this question?\n",
    "\n",
    "To pick a more specific point of interest, let's use these coordinates of San Juan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a46609-1899-40fa-907c-2e99fd8dfe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "POI = point(-66.063889, 18.406389, crs='EPSG:4326')\n",
    "\n",
    "# Let's find out where the point is\n",
    "point_map = POI.explore(name='point')\n",
    "point_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec078acb-bfb7-4af6-9215-6fadd5babb99",
   "metadata": {},
   "source": [
    "## Finding data\n",
    "\n",
    "Now that we know where to look, we need to find some data we can use to answer this question. Turns out Microsoft's Plantary Computer has zarr data, and [we can search for zarr to find possible datasets](https://planetarycomputer.microsoft.com/catalog?filter=zarr). A prime contender of the options is the [Daymet Daily Puerto Rico](https://planetarycomputer.microsoft.com/dataset/daymet-daily-pr) dataset. For the sake of keeping this exercise simple, let's just consider measurements from 2020 (the last year in the dataset), and we'll sum the daily maximum temperature (`tmax` variable).\n",
    "\n",
    "Zarr datasets can be represented by a STAC collection without items, with a collection asset pointing to the zarr root. To get more information about this dataset we can fetch the `daymet-daily-pr` STAC collection from Planetary Computer. Pay special attention to the `assets`, `cube:variables`, and `cube:dimensions` attributes of the fetched collection.\n",
    "\n",
    "Note that accessing Planetary Computer data from Azure Blob Storage requires a signed token; we can use the `planetary_computer` package as a \"plugin\" of sorts to `pystac_client` to ensure we get an access token attached as necessary to each asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0f4ba-b68a-4063-b4cf-371add0d3e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")\n",
    "collection = catalog.get_collection('daymet-daily-pr')\n",
    "collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d335296-c3ec-477f-a789-81be915e63fe",
   "metadata": {},
   "source": [
    "## Accessing the zarr files\n",
    "\n",
    "We're going to use `fsspec` with the `abfs` filesystem type (as provided by the `adlfs` package) to get access to the zarr directory tree and its files. The `abfs` filesystem requires both an access token signed to and the Azure account name of the bucket we are attempting to access. Because we used the `planetary_computer` package with `pystac_client` when querying the collection, these connection details are included in the `xarray:storage_options` property of the `zarr-abfs` asset, and we can use them to successfully initialize our filessytem object `fs` and use it to explore the contents of the bucket.\n",
    "\n",
    "All the paths we're interested in will be under the root of the zarr we are interested in. That path is provided within the `zarr-abfs` asset's href."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c563e5-3259-4188-8104-f7ea8df35572",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset = collection.assets[\"zarr-abfs\"]\n",
    "asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f7d2c8-30e0-4d1e-aa6f-ae23d0cf86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the href without the `abfs://`\n",
    "zarr_root = Path(asset.href.split('//', 1)[1])\n",
    "zarr_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e64b6f-51c8-4ede-a183-a87a13655aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the xarray:storage_options gives us the access token and account name required to connect\n",
    "fs = fsspec.filesystem('abfs', **asset.extra_fields['xarray:storage_options'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121b9d98-6059-4652-a5a8-227823238da0",
   "metadata": {},
   "source": [
    "An fsspec filesystem provides an API that includes normal filesystem-related functions, such as listing a path (`ls`), reading files (`open), or finding file information (`stat`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3719ff-9b97-4795-8f46-812264d1ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can list the zarr root\n",
    "fs.ls(str(zarr_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a33c523-b57c-46d6-a83f-1b5ebd1e98f6",
   "metadata": {},
   "source": [
    "Note that when we list the zarr root we see some `.z*` files with zarr-related metadata and othe such info, which can be used to understand how to access the data in the listed directories. Notice also that the listed directory names predominately map to the zarr variables listed out in the STAC collection. From this we can be reasonably certain that the data for each varaible is nested within the directory of the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099bd874-4d2c-4e51-8784-99a1911e2b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we can read a file\n",
    "with fs.open(str(zarr_root / '.zattrs')) as f:\n",
    "   content = f.read()\n",
    "\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15119484-9d65-44f5-8a47-49a3f2b47d53",
   "metadata": {},
   "source": [
    "## Making browsing even easier\n",
    "\n",
    "Even with fsspec, browsing and reading files has a bit more boiler plate than we might like. We can easily create some simple functions to make an easier API for the types of operations we need to do to explore our zarr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8004be-c846-494b-bc04-e01715c39230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make some convenience functions to make browsing easier\n",
    "def ls_zarr(path: str) -> list[str]:\n",
    "    return fs.ls(str(zarr_root / path))\n",
    "\n",
    "def read_zarr_file(path: str) -> bytes:\n",
    "    with fs.open(str(zarr_root / path)) as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_zarr_json(path: str) -> dict[str, Any]:\n",
    "    return json.loads(read_zarr_file(path))\n",
    "\n",
    "def print_json(_json: dict[str, Any]) -> None:\n",
    "    print(json.dumps(_json, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31ee76a-4aec-440a-93c3-ff1277edcdbf",
   "metadata": {},
   "source": [
    "And we can use one of the convenience functions to show much simpler reading the zarr metadata can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a3c3a9-e0a3-49fe-8b09-a722488fe837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zmeta = read_zarr_json('.zmetadata')\n",
    "print_json(zmeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4597588d-ebf4-45b1-abfd-29a05779ae1b",
   "metadata": {},
   "source": [
    "### `.zmetadata`\n",
    "\n",
    "Notice that the `.zmetadata` file starts with the `.zattrs` key, the value of which contains the same content as the `.zattrs` file we read a moment ago? Spoiler alert: `.zmetadata` contains the contents of all the `.z*` files. It's a great way to get an overview of the entire contents of the zarr. From here we can see a full inventory of the `.z*` file types through the archive:\n",
    "\n",
    "* `.zmetadata`: this file only\n",
    "* `.zattrs`: top-level and one in each variable's directory\n",
    "* `.zgroup`: top-level, just specifies the zarr version (here version 2)\n",
    "* `.zarray`: one in each variable's directory\n",
    "\n",
    "Note that `.zmetadata` appears only to be present if the zarr has \"consolidated metadata\", as this one does (and apparently that is a zarr extension and not part of the core spec).\n",
    "\n",
    "### `.zattrs`\n",
    "\n",
    "`.zattrs` appears to provide user-facing information, such as dataset version information, citations, and that which can help users appropriately interpret the meaning of the data including units, array dimensions, and the like. Note that the collection also contains much of this information. For our purposes these values can help us know which files we might want to look at and how to interpret their structure and values, but otherwise we will be programmatically skipping over them.\n",
    "\n",
    "### `.zgroup`\n",
    "\n",
    "This is super relevant to zarr readers to know what format the zarr is. For us, we are just exploring whatever we've found here, so it's not relevant really at all. At least not until we get to trying to read the next zarr file and we start wondering why it looks different than this one...\n",
    "\n",
    "### `.zarray`\n",
    "\n",
    "Finally, a good one. We're going to need to be very interested in the `.zarray` files because they tell us some key information we're going to need to properly read the data arrays. Specifically, the fields data type, compression type (note that all are compressed via the `blosc` algorithm with the same settings), and chunk size are ones we're absolutely going to need. Some other fields that could be very relevant for reading data in other files include `filters` and `fill_value`, but we won't need to be concerned with those here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d517f-ad3f-4b54-b039-59741b3c2be2",
   "metadata": {},
   "source": [
    "## Reading an array\n",
    "\n",
    "We're going to need location information, and our POI is in WGS84 (lat/long), so maybe we start by reading the `lat` variable?\n",
    "\n",
    "First, let's see what all is in the `lat` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad52ef08-736a-43f6-b570-b916135cb73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_zarr('lat/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6500590c-4155-4723-ae71-049bed939bc1",
   "metadata": {},
   "source": [
    "Not a lot. Maybe if we look at the `.zarray` and `.zattrs` we can learn something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2799a5-37ae-445b-bb62-784e19d4be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_zarray = read_zarr_json('lat/.zarray')\n",
    "print_json(lat_zarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0048f9e7-8c6d-4258-9209-2d0646105fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_zattrs = read_zarr_json('lat/.zattrs')\n",
    "print_json(lat_zattrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ce163-2ba1-41ec-aae4-8dbd2c121229",
   "metadata": {},
   "source": [
    "The contents of both of these are exactly what we saw in top-level `.zmetadata`. No suprise there. And the `.zattrs` contents aren't particularly helpful at the moment.\n",
    "\n",
    "But the `.zarray`, that explains some things. First, we see the chunk size (`chunks`) is equal to the overall array size (`shape`). This tells us that this particular variable has only one chunk covering the entire extent of the data set to which it is relevant. We can make an educated guess then that `lat/0.0` is the array data for the single chunk.\n",
    "\n",
    "Let's read it and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e59aa-c0f9-4cd8-96ab-450794f980fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_bytes = read_zarr_file('lat/0.0')\n",
    "print(f'{lat_bytes[:100]}...')\n",
    "print(len(lat_bytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613e6a78-1da7-4426-99de-5233e888ad1d",
   "metadata": {},
   "source": [
    "Just a bunch of ugly binary data--as we might have suspected, espcially given that it is supposedly compressed.\n",
    "\n",
    "Looking into the `blosc` compression format appears to indicate that it is `lz4`, so it seems like an `lz4`-compatible codec would be required. It turns out that's not entirely true. To cut a long story short, digging into this and trying to unravel what something like xarray uses to decompress such an array leads to [the `numcodecs` package](https://github.com/zarr-developers/numcodecs). It has a `blosc` codec, which can use to decompress this byte string and get an array.\n",
    "\n",
    "We'll be doing that operation a bunch, so let's make another convenience function that we can use to quickly and easily read these compressed array files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a58d1-22bf-42a9-8455-38def2649e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numcodecs.blosc\n",
    "\n",
    "def read_zarr_blosc(path: str) -> Any:\n",
    "    #with lz4.frame.LZ4FrameDecompressor() as decompressor:\n",
    "    return numcodecs.blosc.decompress(read_zarr_file(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c9881a-b136-418a-a54b-ab13baaca1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_uncompressed = read_zarr_blosc('lat/0.0')\n",
    "print(f'{lat_uncompressed[:100]}...')\n",
    "print(len(lat_uncompressed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f9090-7985-40ba-96e5-8f2d8c3e6e60",
   "metadata": {},
   "source": [
    "Uncompressed it's still pretty ugly, but again not really unexpected because it's binary data. We need to unpack it into an array and see what that looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3122c97-635f-4d76-9e2a-40de7b3c096f",
   "metadata": {},
   "source": [
    "#### A quick note on data types\n",
    "\n",
    "The format of the data type is not what we might suspect after all the work we did with the `struct` package and packing/unpacking the TIFF values. Apparently the data type specification here is one natively supported by `numpy`, meaning the `dtype` string can be interpreted directly as a data type by `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a444bb8d-f97a-447d-89f4-e6cc81f342f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dtype('<f4'), np.dtype('<f8'), np.dtype('>f2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72fd872-da77-495f-b8b7-2336e431ff77",
   "metadata": {},
   "source": [
    "`numpy` supports unpacking binary data from a byte string natively via the `frombuffer` method. We merely need to provide the specified data type for it to unpack the bytes properly, and then we can reshape the resulting array into the `chunks` shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6a1f6-63b1-47d5-87e2-3359e6c12e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = np.dtype(lat_zarray['dtype'])\n",
    "lat_array = np.frombuffer(lat_uncompressed, dtype=dt).reshape(lat_zarray['chunks'])\n",
    "lat_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b119229d-0a60-4f65-9241-95c956cf3141",
   "metadata": {},
   "source": [
    "Success! Those values should look like what we'd expect for latitude values in or around Puerto Rico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8665e25b-bc84-4238-b354-7687bcc06a18",
   "metadata": {},
   "source": [
    "We can read in the `lon` variable the exact same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b94a3-849f-4809-82ab-a35930165785",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_zarray = read_zarr_json('lon/.zarray')\n",
    "dt = np.dtype(lon_zarray['dtype'])\n",
    "lon_array = np.frombuffer(read_zarr_blosc('lon/0.0'), dtype=dt).reshape(lon_zarray['chunks'])\n",
    "lon_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f88c32-27e8-49b5-884c-c0c4afd51074",
   "metadata": {},
   "source": [
    "## Finding the cell coordinates of our POI\n",
    "\n",
    "Now that we have the `lat` and `lon` variable arrays, we should be able to locate the cell coordinates corresponding to our POI, right?\n",
    "\n",
    "Well, if we look closely at the arrays we realize something: they are two-dimensional! Neither latitude nor longitude are actually one of the base dimensions of our data cube. If we look more closely at the collection's metadata we realize that the data are in a projected coordinate system. We _could_ presumably use the `lat` and `long` variable arrays to find our cell coordinates, but doing so would require some sort of non-obvious multivariate optimization problem.\n",
    "\n",
    "Instead, what happens if we open our `x` variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ffa22e-33b5-4634-a5ac-c074883e86a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_zarray = read_zarr_json('x/.zarray')\n",
    "dt = np.dtype(x_zarray['dtype'])\n",
    "x_array = np.frombuffer(read_zarr_blosc('x/0'), dtype=dt).reshape(x_zarray['chunks'])\n",
    "x_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a91e0e-f561-42d8-91bb-59efdfd3bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5318e8-f741-41ba-a515-77e13ca97c6a",
   "metadata": {},
   "source": [
    "Oh, look at that, it's a one-dimensional array of the cell x coordinates in the projected coordinate system.\n",
    "\n",
    "Let's look at `y` too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b9f84-b357-4138-86a7-5e1bdcebbdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_zarray = read_zarr_json('y/.zarray')\n",
    "dt = np.dtype(y_zarray['dtype'])\n",
    "y_array = np.frombuffer(read_zarr_blosc('y/0'), dtype=dt).reshape(y_zarray['chunks'])\n",
    "y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e9afc-860b-4285-b05b-1b0171be5646",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85e595-379c-41f9-a771-0da4509b17f9",
   "metadata": {},
   "source": [
    "`y` is one-dimensional as well. Not only that, but the shapes of each map to the opposing dimensions of our `lat` and `lon` array shapes. Seems like we're on the right track here, if we only could convert our POI into the projected coordinates..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa9e46a-e7ef-4ec7-96fe-419ddc20de9b",
   "metadata": {},
   "source": [
    "### Understanding our reference system\n",
    "\n",
    "One outstanding problem with zarr is that the geospatial extension still has yet to be formalized. This means we don't have a built-in way of grabbing the CRS information from the zarr itself. Planetary Computer gets around this issue by storing the CRS information in the STAC collection, specifically as part of the `cube:dimensions` metadata.\n",
    "\n",
    "Both the `x` and `y` dimensions have a reference system defined. Thankfully for us, the data provide has made a sane decision to ensure these CRSs are the same, so we only need to use one of them to get a transformer we can use to project our POI coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa8aac7-9874-4182-b054-4a0fbb0914a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.extra_fields['cube:dimensions']['x']['reference_system']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4ba3c-cebf-4596-ba8e-b49089996911",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_crs = pyproj.CRS.from_json_dict(collection.extra_fields['cube:dimensions']['x']['reference_system'])\n",
    "src_crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792bbbc3-d62d-4f2b-9288-ae71146b2b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_src_transformer = pyproj.Transformer.from_crs(\"EPSG:4326\", src_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1d6506-057a-4806-a7e5-8cf5494e527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_projected_x, poi_projected_y = to_src_transformer.transform(\n",
    "    POI.geom.y,\n",
    "    POI.geom.x,\n",
    ")\n",
    "poi_projected_x, poi_projected_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab40418-6b9e-44b4-8978-ec3da150ec06",
   "metadata": {},
   "source": [
    "### Calculating our cell coords with a \"geotransform\"\n",
    "\n",
    "At this point we're really just in regular raster land and find outselves needing the raster grid's affine transformation--or geotransform in GDAL speak--to convert our coordinates from model space to grid space. Other than having to continue to pull metadata out of the collection to get our inputs, we can calculate the values we need to create that data structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e5839d-ce79-4aa4-9f42-ffe129578b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "geotransform = (\n",
    "    collection.extra_fields['cube:dimensions']['x']['extent'][0],\n",
    "    collection.extra_fields['cube:dimensions']['x']['step'],\n",
    "    0,\n",
    "    collection.extra_fields['cube:dimensions']['y']['extent'][1],\n",
    "    0,\n",
    "    collection.extra_fields['cube:dimensions']['y']['step'],\n",
    ")\n",
    "geotransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0f9a05-4db6-4511-99f8-f4fad323f555",
   "metadata": {},
   "source": [
    "With that, we can finally do the normal conversion of geographic coordinates into grid coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f2ddd4-b208-4d2b-8939-fbc00113104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "POI_col = int((poi_projected_x - geotransform[0]) // geotransform[1])\n",
    "POI_row = int((poi_projected_y - geotransform[3]) // geotransform[5])\n",
    "POI_row, POI_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ceb605-9223-4d8a-b7c8-3e1aee83f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_array[POI_row, POI_col], lat_array[POI_row, POI_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de0f372-cfa8-4cf8-a344-4af51fa9939d",
   "metadata": {},
   "source": [
    "Does that look right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628798ca-ddff-429d-952c-d53c12038e4a",
   "metadata": {},
   "source": [
    "### Another option to find the cell coords\n",
    "\n",
    "Remember that the `x` and `y` arrays are one-dimensional and increasing/decreasing by a constant step? We could have also used that property to perform a linear search of the coordinate space to get our row and column coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722bf1a-f339-432c-aa76-ae2533993f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 0\n",
    "POI_row_ = None\n",
    "while poi_projected_y < y_array[col]:\n",
    "    POI_row_ = col\n",
    "    col += 1\n",
    "POI_row_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6258744-2ca8-4d83-8129-ee772d18c26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 0\n",
    "POI_col_ = None\n",
    "while poi_projected_x >= x_array[col]:\n",
    "    POI_col_ = col\n",
    "    col += 1\n",
    "POI_col_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29176774-f1de-4fe8-8dd3-e93c265d041c",
   "metadata": {},
   "source": [
    "Note that this strategy worked in this case because we had all of our x and all our y coordinate values in single arrays. If we were working with a larger zarr dataset this might not have been the case. Also, this strategy is not particularly efficient. As a general rule it is probably best to stick to the coordinate calculation using the offsets from the grid origin rather than this more brute-force strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba838a4-6d17-4422-a321-af7ec0be006e",
   "metadata": {},
   "source": [
    "## Reading the `tmax` variable data\n",
    "\n",
    "If after all this you still remember what we are trying to do, great! If not, it's understandable, so here's a little refresher: we are interested in finding the week of 2020 with the lowest average (mean) `tmax` value. So it seems pertinent that we read in the `tmax` variable data.\n",
    "\n",
    "Let's take a look at what's in the `tmax` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5dbcad-ff8c-42db-9d30-0fb60f6fccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_zarr('tmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6956d-cee7-4c11-b11e-2bb6ca425f2c",
   "metadata": {},
   "source": [
    "Uh oh. This is different than we've run into up till now. We see a bunch of different chunk files here. Maybe if we look at the `.zarray` and `.zattrs` we can figure out something to help us here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02076d0e-531a-4df4-a05b-94a50eb007ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_zarray = read_zarr_json('tmax/.zarray')\n",
    "print_json(tmax_zarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3963bb40-70d3-44a1-9a46-8e98bf65d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_zattrs = read_zarr_json('tmax/.zattrs')\n",
    "print_json(tmax_zattrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d17219-1f4f-4770-b022-0250b54dfd34",
   "metadata": {},
   "source": [
    "We see with `tmax` that the chunk size `(365, 231, 364)` is different than the shape `(14965, 231, 364)`. And from `.zattrs` we see the array dimensions listed as `(time, y, x)`. A deductive person could infer that, this being a daily dataset, the chunk having a size of `365` for the `time` dimension means that each chunk is a year's worth of values. We have 41 chunk files (`0.0.0` though `40.0.0`), and `365 * 41 = 14965`. It further stands to reason that the chunks are in chronological order, so we can reasonably assume we want the last chunk, `40.0.0`.\n",
    "\n",
    "But what good would it do just to be clever here? Instead, let's confirm our theory. To do so we can look at the `time` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7542055e-f718-4d43-93ab-01cabe7bd28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_zarr('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d25b1ff-36cb-4cec-86a7-adfcc3ee4229",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_zarray = read_zarr_json('time/.zarray')\n",
    "print_json(time_zarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539394f1-fcc9-4f80-b386-d218044fb43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "suspected_year_chunk = 2020-1980\n",
    "suspected_year_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a4a2a-8200-4c0e-9b42-038ebb2349a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = np.dtype(time_zarray['dtype'])\n",
    "time_array = np.frombuffer(read_zarr_blosc(f'time/{suspected_year_chunk}'), dtype=dt).reshape(time_zarray['chunks'])\n",
    "time_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6118f-fc13-4b46-a3da-421c17ccf383",
   "metadata": {},
   "source": [
    "Per the metadata for the `time` array, we can interpret these values as the date of that cell in the days since 1980-01-01. We can use the `datetime` module to calculate the date given by cell 0 in the array above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d13b57c-9804-4bd4-8068-6276f366dda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to confirm the dates match up\n",
    "chunk_start_date = datetime.date.fromisoformat('19800101') + datetime.timedelta(days=int(time_array[0]))\n",
    "print(chunk_start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7468a0-ae3d-4a12-bff0-73694ddb4acf",
   "metadata": {},
   "source": [
    "Ah, great, proof that chunk 40 is the chunk we want for 2020 values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4afc2b0-695e-4b73-a4b9-0b131da78dfa",
   "metadata": {},
   "source": [
    "Now that we know which chunk we want, let's finally read it. Note that this array is significantly bigger--365 times bigger, to be exact--than our `lat` and `lon` arrays. As a result, it might take a bit longer to download, extract, and unpack into an array. Be patient. In testing this typically took on the order of 30-40 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba833042-4a58-43f3-923c-a4d5d0553eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = np.dtype(tmax_zarray['dtype'])\n",
    "tmax_array = np.frombuffer(read_zarr_blosc(f'tmax/{suspected_year_chunk}.0.0'), dtype=dt).reshape(tmax_zarray['chunks'])\n",
    "tmax_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0125c4-591e-4a34-b4df-71c77733e64a",
   "metadata": {},
   "source": [
    "What were we trying to do with these values again? Oh yeah, find the average max temperature of each week in the year, so we can identify the minimum. We can do this pretty trivially with `numpy` by:\n",
    "\n",
    "* slicing out only the stack of temperature values we're interested in for our POI cell\n",
    "* reshaping the array into rows of seven values\n",
    "* finding the mean along the row axis (gives us the mean of each week)\n",
    "* find the min value within that weekly mean array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e15085b-e40f-46a3-98f7-d363cf85e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only take 364 temperature values because 365 isn't divisible by 7 üòï\n",
    "POI_tmax_array = tmax_array[:364, POI_row, POI_col].reshape(52, 7)\n",
    "POI_tmax_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac83113-4840-4baf-8ded-ea24bdff1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "POI_tmax_mean_array = np.mean(POI_tmax_array, axis=1)\n",
    "POI_tmax_mean_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b75f06-a333-47ed-bddd-f3944eca1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "POI_tmax_min = np.min(POI_tmax_mean_array)\n",
    "POI_tmax_min_index = np.argmin(POI_tmax_mean_array)\n",
    "print(f'The lowest tmax temp {POI_tmax_min} occurred in week {POI_tmax_min_index} of 2020.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d66a4b4-a018-4f85-a39b-eafe1fb87a89",
   "metadata": {},
   "source": [
    "This is a great result! But it would be good if we could turn that into an actual date. Good thing we read in that `time` array: we can use slice it on weekly bounds to get the date of the first day of the week in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e7984-9c83-4ca7-9f80-7dfecbfd12a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "week__days_since_1980 = int(time_array[::7][POI_tmax_min_index])\n",
    "week__days_since_1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d2add-a54a-4e08-a23c-41af7677a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_week_of_the_year_start = datetime.date.fromisoformat('19800101') + datetime.timedelta(days=week__days_since_1980)\n",
    "print(f'The best day to arrive in Puerto Rico, from our data, was {best_week_of_the_year_start}. Now go book your time machine tickets. üòÅ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
